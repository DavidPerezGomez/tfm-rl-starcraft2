[DEFAULT]
agent_key = single.dqn
model_id = single_dqn
experiment_path = %(models_path)s/best_32

visualize = true

[train]
map = Simple64
num_episodes = 200
early_stopping_interval = 15
save_frequency_episodes = 15

reward_mode = score
score_method = get_game_score_delta

load_agent = false
load_networks_only = false

;dqn properties
dqn_size = large
lr = 0.001
lr_milestones = 150,175
epsilon = 0.99
epsilon_decay = 0.97
min_epsilon = 0.005
batch_size = 512
gamma = 0.9985
main_network_update_frequency = 1
target_network_sync_frequency = 2000
target_sync_mode = soft
update_tau = 0.1

;buffer properties
memory_size = 25000
burn_in = 5000

[train.opponent]
agent_key = single.random
model_id = single_random

map = Simple64

reward_mode = score
score_method = get_game_score_delta

[exploit]
map = Simple64
num_episodes = 100

reward_mode = score
score_method = get_game_score_delta

load_agent = true
load_networks_only = false

[exploit.opponent]
agent_key = single.random
model_id = single_random

map = Simple64

reward_mode = score
score_method = get_game_score_delta
